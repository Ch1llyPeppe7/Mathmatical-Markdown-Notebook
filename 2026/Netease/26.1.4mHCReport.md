## 残差连接与超连接
::: **
给定一个任意$L$层同质深层网络的**层映射**

$$S_l:X_l \rightarrow X_{l+1},$$假设其形式可为
$$\begin{align}
    单纯嵌套：&x_{l+1}=F(x_l,W_l),\\
    残差连接：&x_{l+1}=x_l+F(x_l,W_l),\\
    超连接：&\begin{cases}
        \tilde x_{l+1}=H^{res}_l  \tilde x_l+{{H_l^{post}}^T}F(H_l^{pre}  \tilde x_l,W_l),\\
         \tilde x_0=E\circ x_0
    \end{cases}
\end{align}$$其中初始特征量$x_0\in X_0\subset \mathbb{R}^d,$
$E:\mathbb{R}^d\rightarrow \mathbb{R}^{n\times d}$为任意扩张算子，可以是复制或线性投影张量$E\in \mathbb{R}^{d\times n\times d},$
$H^{res}_l\in \mathbb{R}^{n\times n},W_l\in \mathbb{R}^{d\times d}$是分别是$\mathbb{R}^{n}和\mathbb{R}^{d}$上任意同态映射，
${H_l^{post}},H_l^{pre}\in \mathbb{R}^{1\times n}$分别为嵌入（其对偶形式${H_l^{post}}^T$）、投影映射。
::: **Remark 1**
**残差连接**的初衷是改变层映射的一阶行为，通过增加**恒等映射**以预防梯度消失问题;形式上在零阶行为里**直接混合**了原始特征，可能会带来**特征稀释**的问题，即原始特征主导了**层输出**，使得**特征变换**难以做出贡献。
**超连接**是残差连接的推广，他将原来的**单一路径**推广到**多路径**，并学习路径两种**混合策略**策略。在**层映射**上同时有多个网络在并行探索，对应地需要学习一组**交换矩阵**（$H^{res}$）实现**横向记忆交换**；在**特征变换**上各子网络记忆被聚合后统一变换，再分发回各网络。
:::

::: **Remark 2**
**同质性意为网络中任意层映射$S_i$的特征变换$F_i$形式上都是相同的函数**
:::
::: **Remark 3**
**超连接的形式像是网络传播方向上的状态空间模型(e.g. Mamba)**,其基本抽象形式为$$\begin{align}
    h_{t+1}&={A}{h_t}+Bu_t,\\
    y_t&=Ch_t+Du_t,\\
\end{align}$$
可进一步变换为
$$\begin{align}
    y_{t+1}=C\circ Ah_t+(C\circ B+D)u_t
\end{align}$$
令$(6)定义了映射T：U\rightarrow Y,y_{t+1}=T(u_t)不妨令U\cong Y \cong X$,则$T$诱导了一个形式上类似超连接的空间序列$\set {X_i}^L_0$
这里不再展开讨论。
:::
:::
## 残差流
**残差流**指深层网络特征增量序列$\set {\Delta x_i}_{1}^{L}$,其中$\Delta x_i=x_i-x_{i-1}$
单纯嵌套网络的残差流分析较为繁琐；当层映射含有残差连接时，其**特征变换输出量序列**即为**残差流**，这使得我们可以把带残差连接的深层神经网络看作一个**离散步长的动力系统**，通过分析特征变换的谱，可以讨论网络的**收敛性**、**稳定性**等性质以设计更易于训练的网络结构。
$$\begin{align}
    x_L&=x_l+\sum_{i=l}^{L-1} F(x_i,W_i)\\
\end{align}$$
$S$的一阶导函数，也即$X_l,X_{l+1}之间的切映射$
$$s:T_{x_l}X_l\rightarrow T_{S_l(x_l)}X_{l+1}$$其形式为$dS(x_l)\approx s(x_l)·dx_l$,其中$dx_l$为$X_l$上$x_l$处的无穷小量,在三种形式下分别对应
$$\begin{align}
    s_l&=\frac{\partial {F}}{\partial{x_{l}}}\\
        s_l&=I+\frac{\partial {F}}{\partial{x_{l}}}\\
        s_l&=H^{res}_l+\frac{\partial {F}}{\partial
        (H_l^{pre}{x_{l}})}\circ H_l^{pre}
\end{align}$$

我们忽略损失泛函$\mathcal{L}(x_L,\Phi)$与层映射$S(x_l,W_l)$的具体形式，关注优化过程$$\frac{\partial {\mathcal{L}}}{\partial{W_i}}=\frac{\partial {\mathcal{L}}}{\partial{x_L}}·\frac{\partial {x_L}}{\partial{S_i}}·\frac{\partial {S_i}}{\partial{W_i}}$$
受到深层网络结构影响的部分：$$\frac{\partial {x_L}}{\partial{S_i}}=\prod ^{L-1}_{j=i} \frac{\partial {S_{j+1}}}{\partial{S_{j}}}=\prod ^{L}_{j=i+1} s_j $$


:::