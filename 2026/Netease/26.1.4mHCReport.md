<blockquote class="cover-page">
  <div class="cover-title">残差网络系列研究</div>
  <blockquote class="cover-subtitle">
  深度残差、宽度残差与超残差
  </blockquote>
  <div class="cover-meta-group">
    <div class="cover-author">Jin.Qian</div>
    <div class="cover-date">2026.1.5</div>
  </div>
</blockquote>

<blockquote class="chapter">

# 残差连接与超连接
给定一个任意$L$层同质深层网络的**层映射**
<blockquote class="definition">

$$S_l:X_l \rightarrow X_{l+1},$$假设其形式可为
  $$\begin{align}
    单纯嵌套：&x_{l+1}=F(x_l,W_l),\\
    深度残差：&x_{l+1}=x_l+F(x_l,W_l),\\
    超连接：&\begin{cases}
        \tilde x_{l+1}=H^{res}_l  \tilde x_l+{{H_l^{post}}^T}F(H_l^{pre}  \tilde x_l,W_l),\\
         \tilde x_0=E\circ x_0
    \end{cases}
\end{align}$$

</blockquote>

其中初始特征量$x_0\in X_0\subset \mathbb{R}^d,$
$E:\mathbb{R}^d\rightarrow \mathbb{R}^{n\times d}$为任意扩张算子，可以是复制或线性投影张量$E\in \mathbb{R}^{n\times d\times d},$
$H^{res}_l\in \mathbb{R}^{n\times n},W_l\in \mathbb{R}^{d\times d}$是分别是$\mathbb{R}^{n}和\mathbb{R}^{d}$上任意同态映射，
${H_l^{post}},H_l^{pre}\in \mathbb{R}^{1\times n}$分别为嵌入（其对偶形式${H_l^{post}}^T$）、投影映射。
关于$DenseNet$的讨论将放在后面的小节。

<blockquote class="remark">
  
**残差连接**的初衷是改变层映射的一阶行为，通过增加**恒等映射**以预防梯度消失问题;形式上在零阶行为里**直接混合**了原始特征，可能会带来**特征稀释**的问题，即原始特征主导了**层输出**，使得**特征变换**难以做出贡献。
**超连接**是残差连接的推广，他将原来的**单一路径**推广到**多路径**，并学习路径两种**混合策略**策略。在**层映射**上同时有多个网络在并行探索，对应地需要学习一组**交换矩阵**（$H^{res}$）实现**横向记忆交换**；在**特征变换**上各子网络记忆被聚合后统一变换，再分发回各网络。

  
</blockquote>

<blockquote class="remark">
  
**同质性意为网络中任意层映射$S_i$的特征变换$F_i$形式上都是相同的函数**

</blockquote>

<blockquote class="discussion">
  
**超连接的形式像是网络传播方向上的状态空间模型(e.g. Mamba)**,其基本抽象形式为$$\begin{align}
    h_{t+1}&={A}{h_t}+Bu_t,\\
    y_t&=Ch_t+Du_t,\\
\end{align}$$
可进一步变换为
$$\begin{align}
    y_{t+1}=C\circ Ah_t+(C\circ B+D)u_t
\end{align}$$
令(6)定义了映射$T：U\rightarrow Y,y_{t+1}=T(u_t)$,
不妨令$U\cong Y \cong X$,
则$T$诱导了一个形式上类似超连接的空间序列$\set {X_i}^L_0$
这里不再展开讨论。

</blockquote>

</blockquote>
<blockquote class="chapter">

# 残差流
**残差流**指深层网络特征增量序列$\set {\Delta x_i}_{1}^{L}$,其中
<blockquote class="definition">
  
  $$\Delta x_i=x_i-x_{i-1}$$

</blockquote>

单纯嵌套网络的残差流分析较为繁琐；当层映射含有残差连接时，其**特征变换输出量序列**即为**残差流**，这使得我们可以把带残差连接的深层神经网络看作一个**离散步长的动力系统**，通过分析特征变换的谱，可以讨论网络的**收敛性**、**稳定性**等性质以设计更易于训练的网络结构。
将式$(2)$错位相减可以得到$$\begin{align}
    x_L&=x_l+\sum_{i=l}^{L-1} F(x_i,W_i),\\
\end{align}$$$S_l$的一阶导函数，也即$X_l,X_{l+1}之间的切映射$
$$s_l:T_{x_l}X_l\rightarrow T_{S_l(x_l)}X_{l+1},$$其形式为$$s_l(x_l)[v]:= \frac{d}{dt}S_l(x_l+tv)\mid _{t=0}=\frac{\partial x_{l+1}}{\partial x_{l}}\circ v,$$$(1)、(2)、(3)$三种形式选定局部坐标$x_l$后分别对应
<blockquote class="definition">



$$\begin{align}
    s_1(x_l)&=\frac{\partial {F}}{\partial{x_{l}}},\\
        s_2(x_l)&=I+\frac{\partial {F}}{\partial{x_{l}}},\\
        s_3(x_l)&=H^{res}_l+{{H_l^{post}}^T}\circ \frac{\partial {F}}{\partial
        (H_l^{pre}\tilde{x_{l}})}\circ H_l^{pre},\\
\end{align}$$
</blockquote>

<blockquote class="remark">
  

在选定局部坐标$x_l$并忽略$H^{pre}_l\circ Expand(x_l)$带来的度量畸变时，$\frac{\partial {F}}{\partial
        (H_l^{pre}\tilde{x_{l}})},\frac{\partial {F}}{\partial{x_{l}}}$具有同源谱结构。


</blockquote>

注意到情形$(8)、(9)$，我们只需要关注特征变换本身的形式、谱特征就能控制好整个网络，而情形$(10)$我们需要额外关注${H}$的性质。
我们忽略损失泛函$\mathcal{L}(x_L,\Phi)$与层映射$S(x_l,W_l)$的具体形式，关注优化过程$$\frac{\partial {\mathcal{L}}}{\partial{W_i}}=\frac{\partial {\mathcal{L}}}{\partial{x_L}}\circ \frac{\partial {x_L}}{\partial x_{i+1}}\circ \frac{\partial {x_{i+1}}}{\partial{W_i}}$$
受到深层网络结构影响的部分：

<blockquote class="objective">
  
$$\frac{\partial {x_L}}{\partial{x_{i+1}}}=\prod ^{L-1}_{j=i+1} \frac{\partial {x_{j+1}}}{\partial{x_{j}}}=\prod ^{L-1}_{j=i+1} s_j ,$$

</blockquote>

代入$(8)、(9)$容易发现

<blockquote class="definition">
  


$$
\begin{align}
    J_1&=\prod_{j=i+1}^{L-1} \frac{\partial {F}}{\partial{x_{j}}} \\
    J_2&=I+\sum _{j=i+1}^{L-1} \frac{\partial {F}}{\partial{x_{j}}}+\sum _{i+1\le j_1\le j_2\le L-1} \frac{\partial {F}}{\partial{x_{j_2}}}\circ \frac{\partial {F}}{\partial{x_{j_1}}}+...+J_1\\
    J_3&=\prod _{j=i+1}^{L-1} H^{res}_j+...
\end{align}
$$

</blockquote>



<blockquote class="proposition">
  
  
$$J_2\approx exp(\sum _{j=i+1}^{L-1} \frac{\partial {F}}{\partial{x_{j}}})$$
待补充，可以从丛论角度解释HC

</blockquote>

</blockquote>

<blockquote class="chapter">


# 路径范畴
在上一章节，我们了解了深度网络训练的障碍，并且介绍了几种已有的解决思路，即残差结构。然而，我们仍未知晓这几种结构的本质动机，以及他们之间的关系。在这一章节，我们将以**特征空间**与**特征映射**为主要对象，探讨在残差结构下网络的特征表达与优化性质究竟发生了什么变化。
<blockquote class="section">

## 单纯嵌套深度网络
对于一个单纯嵌套的$L$层深度网络，其可以看作如下组合映射系统

<blockquote class="definition">
  

$$X_0\xrightarrow{F_0} X_1\xrightarrow{F_1}...\xrightarrow{F_{L-1}} X_L,$$

</blockquote>

其中，$X_i\subset\mathbb{R}^d$,$F_i$为含非线性变换网络作用,不妨假设其为$Transformer$中的$FFN$：

<blockquote class="proposal">
  

$$
X_i\xrightarrow{Emb}Z_0\xrightarrow{\sigma}Z_1\xrightarrow{Prj}X_{i+1},
$$
</blockquote>

其中，$  X_i,X_{i+1}\subset X\subset \mathbb{R}^d,Z_1\subset Z_0\subset \mathbb{R}^D,d<D$,我们暂且假设FFN表达的空间关系是这样的;
$    Emb\in \mathbb{R}^{D\times d},Prj\in \mathbb{R}^{d\times D}
$分别为左嵌入算子和左投影算子，$\sigma$是任意逐点非线性映射如$Sigmoid,Tanh,ReLU,SiLU$等。

<blockquote class="remark">

逐点激活函数导数通常小于1.这是单纯嵌套深度网络梯度消失的根本原因。

</blockquote>



</blockquote>

<blockquote class="section">

## 深度残差网络(ResNet)
既然在深度网络中难以直接学习**特征变换**，ResNet选择了学习**特征残差**,
<blockquote class="proposal">
  
$$
\begin{CD}
X_0 @>T_0>> X_0+\Delta_0 @>T_1>> X_0+\Delta_0+\Delta_1 @>T_2>> \cdots @>T_{L-1}>>
X_0+\sum_{i=0}^{L-1}\Delta_i \\
@VF_0VV     @VF_1VV     @VF_2VV\\
\Delta_0  @. \Delta_1 @.\Delta_2
\end{CD}$$
</blockquote>

不难看出，层映射被一个由**特征映射**诱导的**平移变换**取代了。直觉上来说，同样一个特征变换需要更多次层映射表达，这就是提升训练动力的代价。与**特征稀释**的说法相比，这种视角揭示了残差网络的表达形式，我们无需考虑每一层的混合效应，因为更根本的变化是网络的学习模式发生了变化。
然而，带残差结构的网络往往性能会更好，我们应当想到其中存在一个混淆变量——原网络浅层变换难以优化——而不是认为残差网络具有更好的表达能力。
<blockquote class="remark">
  

深度残差网络ResNet实际上定义了一种新的系统,特征映射退化为局部坐标到平移向量场的映射，整体的层映射退化为一种扩散行为，本质上牺牲了表达能力(需要更多的层来表示旋转、仿射等变换)换取训练动力、稳定性与收敛性。
</blockquote>
</blockquote>

<blockquote class="section">

## 宽度残差深度网络(DenseNet)
同理，我们先观察$DenseNet$的直接展开形态。
<blockquote class="proposal">
  

$$
\begin{align*}
    X_0\xrightarrow{F_0} X_1\xrightarrow{F_1}X_2\xrightarrow{F_2} X_3\xrightarrow{F_3} X_4\xrightarrow{F_4}&...\xrightarrow{F_{L-1}} X_L,\\
    X_0\xrightarrow{F_1} X_0^{1!}\xrightarrow{F_2}X_0^{2!}\xrightarrow{F_3} X_0^{3!}\xrightarrow{F_4}&...\xrightarrow{F_{L-1}} X_0^{\frac{{(L-1)}!}{0!}},\\
    X_0\xrightarrow{F_2}X_0^{\frac{2!}{1!}}\xrightarrow{F_3} X_0^{\frac{3!}{1!}}\xrightarrow{F_4}&...\xrightarrow{F_{L-1}} X_0^{\frac{{(L-1)}!}{1!}},\\
     X_1\xrightarrow{F_2}X_1^{\frac{2!}{1!}}\xrightarrow{F_3} X_1^{\frac{3!}{1!}}\xrightarrow{F_4}&...\xrightarrow{F_{L-1}} X_1^{\frac{{(L-1)}!}{1!}},\\
    X_0\xrightarrow{F_3}X_0^{\frac{3!}{2!}}\xrightarrow{F_4} &...\xrightarrow{F_{L-1}} X_0^{\frac{{(L-1)}!}{2!}},\\
    X_1\xrightarrow{F_3}X_1^{\frac{3!}{2!}}\xrightarrow{F_4} &...\xrightarrow{F_{L-1}} X_1^{\frac{{(L-1)}!}{2!}},\\
    X_2\xrightarrow{F_3}X_1^{\frac{3!}{2!}}\xrightarrow{F_4} &...\xrightarrow{F_{L-1}} X_2^{\frac{{(L-1)}!}{2!}},\\
    X_0^1\xrightarrow{F_3}X_1^{\frac{3!}{2!}1!}\xrightarrow{F_4} &...\xrightarrow{F_{L-1}} X_1^{\frac{{(L-1)}!}{2!}1!},\\
    &...\\
    &X_0\xrightarrow{F_{L-1}} X_0^{L-1},\\
    &X_1\xrightarrow{F_{L-1}} X_1^{L-1},\\
\end{align*}
$$

</blockquote>


为了简洁，用指数表达映射对特征的作用$F$按下指标复合。
$DenseNet$最后的输出是对所有末端元素进行聚合，如池化、MLP方法等。

<blockquote class="remark">
  

来自路径范畴的一个朴素结论是：如果我们规定了每层的特征变换，那么整个网络的复杂行为可以被唯一确定。我们也可以考虑设计一种路径等价类压缩方法来缓解网络过宽的问题。
</blockquote>

仅仅从**特征空间**演化的角度看，$DenseNet$的动机是平凡的，它仅仅是过分地重用浅层的特征，我们难以想象末端输出聚合它们的意义。
然而，我们可以换个角度，从**特征变换**的角度，也即考虑它对训练动力的改造。
给定一个任意层序号$l$我们考察它接收了哪些梯度流，容易想到它们是$\bold{Proposal 3.3}$中起点为$F_i$两端**特征空间**的路径,共有$2^{l+1}$条

<blockquote class="proposal">
  
  $$\begin{CD}
    待补充
  \end{CD}$$

</blockquote>


</blockquote>

<blockquote class="section">
  
  ## 双路残差深度网络(DualPathNet)
  不难想到，我们可以综合DenseNet的**高表征能力**与ResNet**高动力下限**,设计一种更强大的方案。具体而言，ResNet的恒等映射部分保证了网络梯度流的回传但对层映射的利用率较低，因为增量学习的范式使得它需要更多层映射来表达长程的变换；而DenseNet通过**特征重用**的方式让每层映射生成了更多的新特征，提高了参数利用率，但显然带来了更高的特征存储开销。
  双路残差网络正是基于这个思路设计的。
  我们之后会补充关于DPN的更多细节。


</blockquote>

<blockquote class="section">
  
  ## 超连接网络(HyperConnection)
  DPN为我们打开了新思路，然而它的双流结构以直和的形式出现，是固定且无交互的。我们不妨再激进一点，如果网络能够自己学习某些层更适合深度残差、另外一些层更适合宽度残差，那么就能在保证梯度流通的情况下通过更强的变换来提升特征表达能力。

  <blockquote class="subsection">
    
  ## HC的流与流混合
  我们不妨倒着从论文中给出的流数$n=4$去猜想，它代表了两条深度流与两条宽度流的混合。
  </blockquote>

</blockquote>



</blockquote>

<blockquote class="chapter">

# 超连接网络的一般形式
在第三章，我们简单讨论了深度残差（ResNet）、宽度残差（DenseNet）解决深度网络训练动力不足的思路，两者的方案都有些极端，后来的双流网络架构（DualPathNet）仍有一定局限。我们不妨考虑一种更灵活的方法：一种全局上自动权衡训练动力和特征表达的架构。
<blockquote class="section">

## 向量丛：底空间，纤维与截面
直观来看，向量丛刻画的是一种“带内部自由度的表示结构”：与其为底空间中的每一个点仅分配一个向量，不如为其分配一个向量空间，从而允许同一语义状态下存在多种并行的表示方式。在这一框架下，所谓“截面”可以理解为一种在每个纤维中选取具体向量的规则。
<blockquote class="definition">

设拓扑空间$X$为底空间，一个纤维丛是三元组 $(\mathcal{E}, \pi, X)$，其中 $\mathcal{E}$ 为总空间，$\pi: \mathcal{E} \to X$ 为投影映射。对任意 $x \in X$，其纤维定义为
$$\mathcal{E}_x:=\pi^{-1}(x),$$
且总空间可以写作所有纤维的无交并
$$\mathcal{E}=\bigsqcup_{x\in X}\mathcal{E}_x,$$
截面$s$是底空间$X$到总空间$\mathcal{E}$的合法提升
$$\Gamma(\mathcal{E}):=\set{s:X\rightarrow \mathcal{E}|\pi (s(x))=x}$$

</blockquote>
<blockquote class="remark">

  当任意$\mathcal{E}_x$都是向量空间，$(\mathcal{E},\pi,X)$是一个**向量丛**。
</blockquote>

<blockquote class="property">

**局部平凡性(𝐿𝑜𝑐𝑎𝑙 Triviality)**
  给定一个纤维丛$(\mathcal{E},\pi,X)$,它是局部平凡的：如果$\forall x\in X,\exist U_x$使，得下图交换

$$\begin{CD}
\pi^{-1}(U_x) @>{\phi}_x>> U_x \times F \\
@V{\pi}VV @VV{\text{proj}_1}V \\
U_x @= U_x
\end{CD}$$
其中$\pi^{-1}(U_x)$是局部总空间，$\phi_x$是同胚映射,也即
$$\forall x \in X,\mathcal{E}_{x}\cong F$$


</blockquote>

<blockquote class="remark">
  
  局部平凡性即总空间$\mathcal{E}$在任意小片段$\mathcal{E}_{U_x}$上看起来像积空间$U_x\times F$，也即小片段里的所有纤维$\mathcal{E}_x$可以用同一个$F$表示，但不保证全局可以统一展开。
</blockquote>
<blockquote class="definition">
  
  $(\mathcal{E},\pi,X)$是平凡丛，如果存在全局平凡化映射
  $$\Phi:\mathcal{E}\rightarrow X\times F,$$
  此时$F$称为典范纤维(Typical Fiber)也即
  $$\mathcal{E}\cong X\times F$$
</blockquote>
</blockquote>
<blockquote class="section">
  
  ## 纤维构造
在考虑超连接网络上的向量丛结构前，我们首先给出纤维$\mathcal{E}_x$的可能形式。
<blockquote class="proposal">

给定底空间$X\subset \mathbb{R}^d$，一种最弱的纤维构造是
$$\mathcal{E}_x\cong\underbrace{\mathbb{R}^d\oplus...\oplus\mathbb{R}^d}_{n~times}\cong\mathbb{R}^{nd}$$
</blockquote>
<blockquote class="remark">

  此时可以将其理解为 $n$ 个 $d$ 维表示的简单拼接。这种形式仅刻画了“多表示并存”，但并未对它们之间的相互关系施加任何结构性约束。
  然而，在超连接网络中，我们不仅希望在同一语义状态下维护多种并行表示，还希望这些表示能够通过可学习的线性算子进行混合、投影与重组。
</blockquote>

<blockquote class="proposal">
  
  带有更更强结构假设的构造是
  $$\mathcal{E}_x\cong\underbrace{\mathbb{R}^d\otimes...\otimes\mathbb{R}^d}_{n~times}\cong\mathbb{R}^{n\times d}$$

</blockquote>
<blockquote class="remark">
 
 这一表示具有明确的结构含义：其中 $\mathbb{R}^n$ 刻画“表示类型”或“分支索引”方向，而 $\mathbb{R}^d$ 则对应具体的特征空间。张量积结构意味着这两个维度并非简单并列，而是允许发生线性耦合，从而支持在“表示类型维度”上的线性混合，同时保持在“特征维度”上的非线性变换。
从这一视角来看，张量积结构并不是对维度的简单重排，而是引入了一种可分离的双线性结构，使得后续定义的算子（如投影算子、聚合算子与残差混合算子）能够自然地解释为沿不同维度作用的线性变换。这一结构正是后文超连接网络中各类线性算子得以统一刻画的基础。
</blockquote>

<blockquote class="discussion">
  
  这里需要强调的是，尽管
  $$dim(\mathbb{R}^{n\times d})=dim(\mathbb{R}^{nd}),$$
  它们的结构是不同的，张量积的概念可以由如下交换图反映的泛性质概括
  $$\begin{CD}
V\times W @>{\beta}>> V \otimes_{\mathbb{K}} W \\
@V{B}VV @VV{\exist !\bar B}V \\
Z @= Z
\end{CD}$$
其中，$V,W,Z$是任意$\mathbb{K}$-线性空间，$B$是双线性映射，$\bar B$是线性映射，张量积空间构造如下
$$V \otimes_{\mathbb{k}} W=\left(\bigoplus_{v \in V,\; w \in W} k \cdot (v \otimes w)\right) \big/ \sim$$
其中，
</blockquote>
</blockquote>
<blockquote class="section">
 
 ## 超连接网络：残差流的丛化 
 我们接下来从向量丛的角度考虑HyperConnection架构的底层逻辑。
首先回顾一下HyperConnection的一个网络无关形式
<blockquote class="definition">
 
  $$
  \begin{align*}
     &\begin{cases}
        \tilde x_{l+1}=H^{res}_l  \tilde x_l+{{H_l^{post}}^T}F_l(H_l^{pre}  \tilde x_l),\\
         \tilde x_0=E\circ x_0
    \end{cases}\\
    &x_0\in X_0\subset \mathbb{R}^d,
E:\mathbb{R}^d\rightarrow\mathbb{R}^{n\times d},
H^{res}_l\in \mathbb{R}^{n\times n},
{H_l^{post}},H_l^{pre}\in \mathbb{R}^{1\times n}
  \end{align*}$$
</blockquote>

我们考察它的残差流
<blockquote class="proposal">
 
  $$
  \Delta_{l}:=\tilde x_{l+1}-\tilde x_{l}=$$
</blockquote>
</blockquote>

</blockquote>
<blockquote class="chapter">

# 超连接残差流的梯度控制问题
不妨假设我们真的需要HC的网络结构，那么其中的$\mathcal{H}^{Res}_l$带来的梯度流稳定性问题显然是值得考察的。

<blockquote class="section">

## 矩阵范数
下面先介绍几种向量$p$-范数定义的矩阵范数。
<blockquote class="subsection">

### 矩阵诱导范数
反映矩阵作为一个线性变换的特征
<blockquote class="definition">

\[
\|A\|_p = \sup_{x \neq 0} \frac{\|Ax\|_p}{\|x\|_p}.
\]
</blockquote>

<blockquote class="example">

- \(p = 2\): 谱范数 (最大的奇异值) \(\|A\|_2 = \sigma_{\max}(A)\)  
- \(p = 1\): 最大列和 \(\|A\|_1 = \max_j \sum_i |A_{ij}|\)  
- \(p = \infty\): 最大行和 \(\|A\|_\infty = \max_i \sum_j |A_{ij}|\)    

</blockquote>

<blockquote class="theorem">
  
 
$$\|A\|_2\le\sqrt{\|A\|_1\|A\|_\infty}$$

</blockquote>

**Theorem 5.1** 为我们提供了一种更简单的梯度控制方案，我们无需进行复杂的奇异值分解和KKT条件，仅需要计算行、列和就能控制线性变换的有界性。然而，我们仍需要进行矩阵乘法得到复合链上每个线性变换的具体形式，这一点可以被双随机矩阵的**闭合性**化简。
</blockquote>

<blockquote class="subsection">

###  Schatten 范数
反映矩阵作为空间上的算子的特性，也称**核范数**。
Schatten范数的定义依赖于矩阵的奇异值
<blockquote class="definition">
  

  \[
\|A\|_{\mathcal S_p} = \left( \sum_{i=1}^{r} \sigma_i(A)^p \right)^{1/p}, \quad 1 \le p < \infty,
\]其中，\(\sigma_1 \ge \dots \ge \sigma_r\) 是\(A\)的奇异值向量。  
</blockquote>

<blockquote class="example">
  

- \(p = 2\): 弗罗贝尼乌斯(Frobenius)范数 \(\|A\|_F = \sqrt{\sum_i \sigma_i^2}\)  
- \(p = \infty\): 谱范数 \(\|A\|_{\mathcal S_\infty} = \sigma_{\max}(A) = \|A\|_2\)  
</blockquote>

</blockquote>
</blockquote>

<blockquote class="section">
  
##  双随机矩阵半群
<blockquote class="warning">
  
  **注意**:最优传输问题中的双随机矩阵可以是非方阵，为了方便我们讨论$HyperConnection$中的自同态映射$\mathcal{H}^{Res}$,这里考虑一种特殊情况。

</blockquote>
<blockquote class="definition">
  

  $\mathcal{D}_n:=\set {\mathcal{A}\in \mathbb{R}^{n\times n}|\forall i,j\in \mathbb{Z}_n,\boldsymbol{1}^T\mathcal{A_{·j}}=\mathcal{A}_{i·}\boldsymbol{1}=1,\mathcal{A}_{ij}\ge0}$
</blockquote>


<blockquote class="theorem">
  
 Birkhoff-Von Neumann Theorem
  $$\mathcal{D}_n=conv\set{\Pi_n},其中\Pi_n:=\set{P\in \mathcal{D}_n|P_{ij}\in \mathbb{Z}_2}$$$\mathcal{D}_n$是含$n!$个$P_i\in\Pi_n$极点的凸多面体。


</blockquote>

<blockquote class="property">
  

  $$\begin{align*}
    &dim(\mathcal{D}_n)=n^2-(2n-1)=(n-1)^2,n>2时\mathcal{D}_n不是单纯形,\\\
    &(\mathcal{D}_n,·)是一个关于矩阵乘法的\bold{幺半群},单位元是I_n,\\
    &置换矩阵群(\Pi_n,·)\cong(S_n,\circ)是(\mathcal{D}_n,·)的\bold{单位群},\\
    &根据\bold{Theorem 5.1},谱范数\|\mathcal{A}\|_2\le1,\\
    &一定存在大小为1的奇异值，特征向量为\boldsymbol{1}_n\\
    &内点\mathcal{D}_n^{\circ}=\set{\mathcal{A}\in \mathcal{D}_n|\mathcal{A}_{ij}>0}=\mathcal{D}_n\cap \mathbb{R}_+^{n\times n}\\
    &唯一不动点、最大内切球中心\mathcal{A}_{Uni}=\frac{1}{n}\boldsymbol{1}_n\boldsymbol{1}_n^T\\
  \end{align*}$$

</blockquote>
现在，我们找到了谱范数有界，且关于矩阵乘法闭合的一族矩阵，我们只需要保证每层线性变换都属于这族矩阵，而无需显示计算复合映射链上每个矩阵的具体形式，更不用进行大规模SVD。

<blockquote class="tip">

我们不妨再考虑$\Pi_n$上关于的凸组合标量作用的空间，可以从复链形的角度，并结合HC中流的意义，考虑如何选择合适的规模参数$n$。
</blockquote>

<blockquote class="discussion">


不难看出，双随机矩阵的矩阵诱导-1范数和矩阵诱导-无穷范数均为1，其谱范数上确界为1；其次，双随机矩阵流形是闭合的，其上任意元素关于矩阵乘法复合仍为双随机矩阵，从而保证了式$(13)$中第一项复合算子作用是不会发散的。我们还可以再详细讨论双随机矩阵的代数、闭合性、有界性、紧致性，并探索$End(n)$中是否存在其他乘法子群也具有这样的谱特征。
注意到$\mathcal{D}_n$的最大奇异值必为1，因此讨论其奇异值下界并考虑施加约束会更有意义。$\mathcal{D}_n$是凸集，单纯同调只能用于证明优化是无拓扑障碍的，这显然能直接从凸性质看出它的各阶连通性；因此可以考虑奇异同调的角度，构造对$\mathcal{D}_n$三角剖分得到单纯形并构造单形间映射与元素到谱元素的映射，得到链复形。
 

</blockquote>
</blockquote>

<blockquote class="section">

## 双随机矩阵的生成方法

<blockquote class="subsection">
  

当我们考虑获取一个双随机矩阵$\mathcal{A}\in\mathcal{D}_n$时，最直接的办法是用$\mathcal{D}_n$的顶点表示
<blockquote class="proposal">

$$\begin{align*}
  &\mathcal{A}=\sum_{i=1}^{n!}\lambda_i·P_i,\\
  其中P_i\in \Pi_n,&\lambda_i\in\mathbb{R}_{\ge0},\sum\lambda_i=1,\forall i,j,P_i\ne P_j,
\end{align*}$$

</blockquote>

这种方法的主要障碍是需要存储$n!$个$\mathbb{R}^{n\times n}$上的元素，尽管置换矩阵是稀疏的，但数量仍造成了困难。
<blockquote class="remark">

  此外，这种表示仅仅是存在性凸分解，同一个$\mathcal{A}$在$span(\Pi_n)$下的表示是不唯一的，因为$dim(\mathcal{D}_n)=(n-1)^2$;而寻找$(n-1)^2$线性无关的$P$似乎仍无法解决问题，这里不展开讨论
</blockquote>

我们需要一种更简单的生成方法，并且希望得到的$\mathcal{A}\in\mathcal{D}_n^{\circ}$以尽量保证**任务损失梯度**不会轻易使得$\mathcal{A}$脱离$\mathcal{D}_n$。
</blockquote>

<blockquote class="subsection">

###  Sinkhorn算法
我们首先考虑在双边边际均为1的最优传输问题中，Sinkhorn构造$\mathcal{A}\in \mathcal{D}_n^{\circ}$的方案。
<blockquote class="algorithm">

**Entropy-Regularized Sinkhorn**

| Step | Description |
|---|---|
| Input | Cost matrix \(C \in \mathbb{R}^{n\times n}_+\), regularization \(\varepsilon>0\), iterations \(T\) |
| Init | \(u \leftarrow \boldsymbol{1},\; v \leftarrow \boldsymbol{1}\) |
| Precompute | \(K \leftarrow \exp(-C/\varepsilon)\) |
| Loop | for \(k = 1,\dots,T\) |
|  | \(u \leftarrow \boldsymbol{1} \oslash (K v)\) |
|  | \(v \leftarrow \boldsymbol{1} \oslash (K^\top u)\) |
| Output | \(\mathcal{A} = \mathrm{diag}(u) K \mathrm{diag}(v)\) |
</blockquote>

其中，$\oslash$是逐元素除法。

<blockquote class="objective">

 \(\mathcal{A}\) is the solution of the entropy-regularized optimal transport problem:
\[
\min_{\mathcal{A} \in \mathcal{D}_n} \langle \mathcal{A}, C\rangle_{\mathcal{F}} - \varepsilon H(\mathcal{A}), \quad H(\mathcal{A}) = -\sum_{i,j} \mathcal{\mathcal{A}}_{ij} \ln \mathcal{A}_{ij}.
\]

</blockquote>
<blockquote class="remark">
  
在HC梯度控制问题中，我们只是想利用双随机矩阵的复合稳定性，似乎并不存在一个明确的代价矩阵。

</blockquote>

任意给定一个$M\in \mathbb{R}^{n\times n}$,我们可以通过逐元素激活函数得到正锥元素$K$,不失一般性的地
<blockquote class="proposal">
  
  $$\begin{align}
    Exponential:K_{ij}&=\alpha^{\tau M_{ij}}\\
    SoftPlus:K_{ij}&=\frac{1}\tau log_{\alpha}(1+\alpha^{\tau M_{ij}})
  \end{align}$$
</blockquote>

通常$\alpha$取自然底数$e$,$\tau$是温度系数,注意到$(15)$的导数是有界的。
我们考虑Sinkhorn对应的优化目标，根据最大熵原理，其中元素级熵正则的作用是使得$\mathcal{A}$逼近一个均匀双随机矩阵$\mathcal{A}_{Uni}=\frac{1}{n}\boldsymbol{1}_n\boldsymbol{1}_n^T$

</blockquote>

</blockquote>

<blockquote class="chapter">

# 实证分析：基于矩阵论的 Sinkhorn 算法收敛性实验

<blockquote class="section">

## 动机：谱结构视角下的 Sinkhorn 动力学分解

本节的基本思想是，将整个建模过程拆分为两个相对独立但逻辑上相互关联的部分：
<blockquote class="proposal">

  1. 从一般实矩阵空间 $\mathbb{R}^{n \times n}$ 到非负实矩阵空间 $\mathbb{R}_{\ge 0}^{n \times n}$ 的**激活映射**；
2. 在非负实矩阵空间上，由 Sinkhorn 算法所诱导的**重整化动力系统**。

</blockquote>


前者刻画的是表示层如何将一般线性结构嵌入到一个具有序结构与正锥约束的空间中；后者则描述了在该空间内，矩阵如何在反复的行列归一化下被“投影”到双随机矩阵流形。

在前述章节中，我们已经讨论过双随机矩阵在深层网络中的意义：其作为一种谱半径为 1 的线性变换，在长复合映射链中起到了抑制梯度爆炸与消失的结构性作用。从这一角度看，Sinkhorn 算法可以被理解为一种离散的谱重整化过程，其目标不是单纯的数值归一，而是将矩阵推向一个谱半径被规范化的结构子空间。

本节的核心问题因此并非：
<blockquote class="info">
 Sinkhorn 是否收敛？
</blockquote>
而是：

<blockquote class="warning">
Sinkhorn 在哪些结构子空间上更快、更稳定、更“自然”地收敛？
</blockquote>


我们将尝试从谱理论的角度刻画这些子空间的几何与代数特征，并为后续激活函数的设计提供可操作的结构依据。

</blockquote>

<blockquote class="section">

## 谱理论简介

在本节中，我们引入若干与非负矩阵谱性质相关的基本概念与定理，以解释 Sinkhorn 迭代在不同结构下可能呈现的动力学行为。这里的目标并非给出完整的谱理论综述，而是建立一个最小但足够的理论框架，使得后续的实验设计、结构分类与指标选取都具有明确的数学动机。

我们将 Sinkhorn 算法视为定义在非负矩阵锥 $\mathbb{R}_{\ge 0}^{n \times n}$ 上的一个离散重整化动力系统。其行为受到两个核心约束：
1. 非负性所诱导的 Perron–Frobenius 结构；
2. 矩阵内部的块结构与谱分离特性。

本节将依次阐释这些结构及其动力学含义。
<blockquote class="subsection">
  
### Perron–Frobenius 结构与主导模态

非负矩阵的谱行为受到 Perron–Frobenius 理论的强烈约束。这一约束决定了 Sinkhorn 动力在主导模态层面上不可能呈现旋转型行为。


<blockquote class="theorem">

**Perron–Frobenius**
设 $A \in \mathbb{R}_{\ge 0}^{n \times n}$ 为非负不可约矩阵，则存在一个实数 $\lambda_{\mathrm{PF}} > 0$，使得：

1. $\lambda_{\mathrm{PF}}$ 是 $A$ 的一个**实特征值**；
2. 对应的特征向量 $x \in \mathbb{R}^n$ 满足
   $$
   A x = \lambda_{\mathrm{PF}} x, \quad x_i > 0 \ \forall i;
   $$
3. 对任意其他特征值 $\lambda \in \sigma(A)$，都有
   $$
   |\lambda| \le \lambda_{\mathrm{PF}}.
   $$

因此，谱半径满足
$$
\rho(A) = \lambda_{\mathrm{PF}},
$$
并且由一个正实特征值实现。

若 $A$ 进一步为本原矩阵，则该特征值的代数与几何重数均为 1。

</blockquote>


该定理的核心含义在于：**非负矩阵的主导模态不携带旋转自由度。**  
即使矩阵存在复特征值，其模也必然被 Perron 根压制，因此不会在长期动力中占据主导地位。

在 Sinkhorn 迭代的语境中，这意味着：

- 算法的主导行为不是“旋转—收缩”的组合，而是**纯尺度型的对齐与归一化**；
- 长期行为必然由 Perron 模态主导；
- 所有复杂行为只能出现在次级模态层面。

这正是非负矩阵空间与一般实矩阵空间的根本差异之一。

</blockquote>

<blockquote class="subsection">

### 可约性结构与动力解耦

除了主导模态之外，矩阵的块结构决定了 Sinkhorn 动力是否会在子系统之间发生解耦，这是影响收敛速度的首要因素。


<blockquote class="definition">

称矩阵 $A \in \mathbb{R}^{n \times n}$ 为**可约的**，若存在置换矩阵 $P$ 使得

$$
P A P^{-1} =
\begin{pmatrix}
B & C \\
0 & D
\end{pmatrix}.
$$

若不存在这样的分解，则称 $A$ 为**不可约的**。

</blockquote>

该定义意味着：在可约情形下，系统可以被拆分为若干相互独立或弱耦合的子系统。

从动力学角度看：

- **真正可约**：Sinkhorn 在每个块内独立收敛；
- **近似可约**：块间存在弱耦合，形成亚稳态；
- **完全不可约**：所有分量强耦合，需全局协调。

因此，可约性并不只是一个代数性质，而是直接决定了 Sinkhorn 动力是否发生**有效维数下降**。

  
</blockquote>
<blockquote class="subsection">
  


### Perron gap 与模态竞争

在不可约情形下，收敛速度主要由谱分离程度决定。

<blockquote class="definition">

设 $A$ 的特征值按模排序为

$$
|\lambda_1| \ge |\lambda_2| \ge \cdots \ge |\lambda_n|.
$$

定义 Perron gap 为

$$
\Delta := |\lambda_1| - |\lambda_2|.
$$

</blockquote>

Perron gap 衡量了主导模态与次主导模态之间的分离程度：

- **强 gap**：主导模态迅速压制其他模态；
- **弱 gap**：存在显著模态竞争；
- **多模态竞争**：多个模态长期共存。

在 Sinkhorn 动力中，这直接表现为：

- 强 gap → 快速单模态对齐；
- 弱 gap → 长平台期；
- 多模态 → 振荡与迟滞。

</blockquote>
<blockquote class="subsection">


### 非对称性与循环结构

非对称性与循环结构不改变极限点，但会显著影响中间态轨道的几何形态。



在谱层面，这些结构主要体现在：

- 特征向量条件数变大；
- 特征值分布偏离实轴；
- 出现近复共轭对。

在 Sinkhorn 轨道中，这些因素通常表现为：

- 迭代路径弯曲；
- Frobenius 距离出现振荡；
- Perron gap 的临时塌缩。

</blockquote>
</blockquote>
<blockquote class="section">
  
## 玩具实验

<blockquote class="subsection">

### 实验指标的谱含义

为了将上述结构转化为可观测量，我们引入以下指标：


| 指标 | 含义 | 对应结构 |
|------|------|----------|
| 行列和偏差 | 是否满足双随机约束 | 是否稳定 |
| 谱半径 $\rho(A_k)$ | 全局尺度 | Perron 结构 |
| Perron gap $\Delta_k$ | 模态分离 | 竞争强度 |
| $\|A_{k+1}-A_k\|_F$ | 局部速度 | 收敛快慢 |
| 谱轨道 | 几何形态 | 旋转/迟滞 |

这些指标并非经验选择，而是谱结构在动力层面的直接投影。

</blockquote>


<blockquote class="subsection">

### 实验设计：谱分类驱动的结构采样

本节的实验并不以“随机抽样”为主，而是以**结构覆盖**为核心目标。我们将通过人为构造具有不同谱特征的 $4 \times 4$ 非负矩阵，来系统性地探测 Sinkhorn 在不同结构子空间上的动力行为。

实验设计遵循以下三个基本轴线：

1. **可约性结构**
   - 完全不可约
   - 近似可约（block + small noise）
   - 真正可约（block diagonal）

2. **Perron gap 结构**
   - 强 gap：$\lambda_1 \gg \lambda_2$
   - 弱 gap：$\lambda_1 \approx \lambda_2$
   - 多模态竞争

3. **非对称与循环结构**
   - 近似对称
   - 强非对称
   - 诱导复特征值的循环结构

在每一类结构下，我们将记录 Sinkhorn 迭代过程中以下量的演化：

- 行列和偏差；
- 谱半径的变化；
- Perron gap 的变化；
- 相邻迭代之间的 Frobenius 距离；
- 中间态的谱轨道。

实验的验证逻辑是：若 Sinkhorn 的动力学确实主要由谱结构决定，则在同一谱类型内部，其轨道形态应呈现出高度一致性；而在不同谱类型之间，则应呈现出显著差异。

</blockquote>

<blockquote class="subsection">

### 实验结果汇总

以下表格给出不同谱结构和可约性类型矩阵的 Sinkhorn(容忍度1e-8) 收敛行为统计结果：

| 矩阵类型                                     | 样本数 | 平均迭代次数          | 收敛率 |
|---------------------------------------------|--------|---------------------|--------|
完全不可约 (密度=0.80)                          | 100    |  10.07 ± 2.76   |  100.0%
近可约 (ε=0.0100)                           | 100    | 287.38 ± 32.27  |  100.0%
真正可约（块对角）                                | 100    |   5.54 ± 1.49   |  100.0%
强Perron gap (比值≈10.0)                    | 100    |   2.00 ± 0.00   |  100.0%
弱Perron gap (gap=12.5737)                | 100    |   8.25 ± 1.36   |  100.0%
多重主导模式 (n=2)                             | 100    |   7.03 ± 2.26   |  100.0%
近对称 (非对称性=0.10)                          | 100    |   8.25 ± 1.36   |  100.0%
强非对称                                     | 100    |  10.95 ± 3.49   |  100.0%
循环转移 (强度=0.80)                           | 100    |  51.15 ± 11.93  |  100.0%

</blockquote>

<blockquote class="remark">

1. **Perron gap 与收敛速度**  
   - 当主特征值与次特征值差距较大（强 Perron gap）时，Sinkhorn 迭代几乎立即进入主模态 → 迭代次数非常少。  
   - 当主特征值间距较小（弱 Perron gap）或存在多重主导模式时，迭代速度相对慢，因为主模态之间存在竞争。

2. **可约性对迭代行为的影响**  
   - 真正可约（块对角）矩阵：每个块独立收敛 → 平均迭代次数较少。  
   - 近可约矩阵：块间存在微弱耦合 → 形成亚稳态 → 迭代次数大幅增加（288 次左右）。  
   - 完全不可约矩阵：存在整体耦合 → 中等迭代次数。

3. **非对称与循环结构**  
   - 轻微非对称对收敛影响有限。  
   - 强非对称会增加迭代次数，但仍可收敛。  
   - 循环结构引入复模态 → 导致中间迭代振荡 → 收敛显著变慢（约 50 次）。

4. **整体结论**  
   - Sinkhorn 在 4×4 非负矩阵空间收敛性良好（100% 收敛率）。  
   - 收敛速度高度依赖谱结构和可约性。  
   - 实验为后续激活函数设计提供结构依据：理想映射应尽量避免“近可约”或“循环模态”输入区域。


</blockquote>
</blockquote>

<blockquote class="section">

## 进一步讨论：从结构收敛到激活函数设计

本节的实证分析并非终点，而是为后续的激活函数设计提供结构依据。若某些谱子空间天然更适合 Sinkhorn 的快速与稳定收敛，则理想的激活函数应当优先将实矩阵映射至这些子空间，而非在整个非负锥中均匀铺展。

从这一角度看，我们真正关心的并不是 Sinkhorn 是否“有效”，而是：

- 它对哪些结构更友好；
- 哪些结构导致慢收敛、振荡或数值不稳定；
- 是否存在可被明确刻画的“好区域”。

这些问题构成了将 Sinkhorn 从一个数值工具转化为结构组件的关键步骤。

</blockquote>

</blockquote>

<blockquote class="discussion">

这里所说的“非负实矩阵空间具有序结构”，并非修辞意义上的描述，而是一个严格的代数事实。具体而言，在 $\mathbb{R}^{n \times n}$ 上可以定义逐元素偏序：
\[
A \preceq B \quad \Longleftrightarrow \quad A_{ij} \le B_{ij} \;\; \forall i,j.
\]
在该偏序下，非负实矩阵集合
\[
\mathbb{R}_{\ge 0}^{n \times n} := \{ A \in \mathbb{R}^{n \times n} : A_{ij} \ge 0 \}
\]
构成一个正锥（positive cone）。这意味着，该空间不仅是一个线性空间的子集，而且携带了一个由非负性诱导的自然序结构。

这一结构性约束对于 Sinkhorn 算法的谱行为具有决定性意义。Perron–Frobenius 理论并不是对一般矩阵成立的结果，而是建立在正锥与正算子的框架之上的：只有在这样的有序空间中，主特征值的正性、正特征向量的存在性以及谱模态的支配性才具有严格意义。

从这一角度看，将一般实矩阵通过激活函数映射至非负实矩阵空间，并不仅仅是为了数值稳定或可解释性，而是为后续的谱分析引入一个可控的代数结构。在该结构中，Sinkhorn 迭代可以被视为一种锥内的重整化流（cone-preserving renormalization flow），其行为受到 Perron 模态与 Perron gap 的显著制约。

因此，本节将非负性视为一种结构性约束，而非简单的取值范围限制。这也是我们能够从谱理论而非仅从数值角度分析 Sinkhorn 动力学的根本原因。

</blockquote>


</blockquote>

<blockquote class="chapter">
  
  # 数值稳定前提下的表征能力与训练动力平衡问题
  为了保证网络的作用与数据的尺度是相容的，从**表征**和**训练**两个角度保证数值稳定性，**归一化层**通常是深度神经网络不可或缺的一个组件。注意到[Derf](https://arxiv.org/pdf/2512.10938v1)在计算机视觉场景下给出了一种无归一化的数值稳定方法，虽然训练误差是被放大的，但模型的泛化能力得到了提升。因此我们称这个网络组件为**数值稳定层**而非归一化层。有时，数据的尺度会承载一定的信息，基于**归一化**的数值稳定方法往往会一定程度上稀释特征信息。
  我们在前面的章节简单讨论了不同残差连接方案在**表征能力**和**训练动力**两个维度反映出来的特性，同样地我们也可以考虑归一化层对这两个属性的影响。需要注意的是，**数值稳定层**和**残差结构**这个两个因素是紧密耦合的，我们通常需要同时考虑它们。
  
  除此之外，也有一些特定的**稳定性机制**来保证优化速度，但我们先重点关注**残差结构**和**数值稳定层**两个方面。
下面先给出一个总结表
<blockquote class="info">
 
 | 维度 | 方案 / 技术 | 表征能力 | 训练动力 |  核心权衡 |
|------|-------------|----------------------------|----------------------------|----------------------|
| **残差结构** | 深度残差（Pre-/Post-Norm） | Post-Norm 维持深层表示多样性；Pre-Norm 易发生表示塌缩 | Post-Norm 梯度消失严重；Pre-Norm 梯度稳定 | **稳定性–表达性跷跷板** |
| | **混合残差（Hyper-Connections）** | 多隐藏态并行，允许跨深度特征集成 | 动态学习串行/并行/混合路径 | **拓扑可学习化** |
| | [**双路残差（ResiDual）**](https://arxiv.org/pdf/2304.14802) | Post-LN 路径提供表示下界 | Pre-LN 路径提供梯度下界 | **路径解耦**：稳定性与表达性并存 |
| **数值稳定层** | **统计归一化（LN / RMSNorm）** | 统计投影约束，抑制表示发散 | 在 Post-Norm 中削弱残差通路 | **分布投影约束** |
| | **逐点映射（DyT / erf系）** | 轻微牺牲拟合，提升几何稳定性 | 平滑梯度，不依赖批量统计 | **几何收缩映射** |
| **稳定性机制** | [**Lipschitz 约束 + 初始化** ](https://arxiv.org/pdf/1911.03179)| 防止表示爆炸 | 改善早期梯度极化 | **函数空间约束** |
| | **DeepNorm / Fixup / Admin** | 控制残差比例 | 控制方差传播 | **尺度对齐** |

</blockquote>
  <blockquote class="section">

  ## 数值稳定层
  

  <blockquote class="subsection">
    
  ### 归一化方法
  <blockquote class="definition">


$$\begin{align}
  LayerNorm(x)=\frac{x-\mu}{\sigma+\epsilon}\odot\gamma+\beta\\
RMSNorm(x)=\frac{x}{\sqrt{\frac{1}{d}\sum_ix_i^2}}  =\sqrt{d}\cdot\frac{x}{\|x\|_2}  
\end{align}$$

  </blockquote>

注意到当$x \sim \mathcal{N}(0, I_d)$，

<blockquote class="lemma">

$$
\lim_{d \to \infty} \mathbb{E}[\|x\|_2] = \sqrt{d}, 
\qquad
\lim_{d \to \infty} \mathrm{Var}(\|x\|_2) = 0.$$


</blockquote>
  </blockquote>
  
  </blockquote>

</blockquote>