<blockquote class="cover-page">
  <div class="cover-title">残差网络系列研究</div>
  <blockquote class="cover-subtitle">
  深度残差、宽度残差与超残差
  </blockquote>
  <div class="cover-meta-group">
    <div class="cover-author">Jin.Qian</div>
    <div class="cover-date">2026.1.5</div>
  </div>
</blockquote>

<blockquote class="chapter">

 # 残差连接与超连接
给定一个任意$L$层同质深层网络的**层映射**
<blockquote class="definition">

$$S_l:X_l \rightarrow X_{l+1},$$假设其形式可为
  $$\begin{align}
    单纯嵌套：&x_{l+1}=F(x_l,W_l),\\
    深度残差：&x_{l+1}=x_l+F(x_l,W_l),\\
    超连接：&\begin{cases}
        \tilde x_{l+1}=H^{res}_l  \tilde x_l+{{H_l^{post}}^T}F(H_l^{pre}  \tilde x_l,W_l),\\
         \tilde x_0=E\circ x_0
    \end{cases}
\end{align}$$

</blockquote>

其中初始特征量$x_0\in X_0\subset \mathbb{R}^d,$
$E:\mathbb{R}^d\rightarrow \mathbb{R}^{n\times d}$为任意扩张算子，可以是复制或线性投影张量$E\in \mathbb{R}^{d\times n\times d},$
$H^{res}_l\in \mathbb{R}^{n\times n},W_l\in \mathbb{R}^{d\times d}$是分别是$\mathbb{R}^{n}和\mathbb{R}^{d}$上任意同态映射，
${H_l^{post}},H_l^{pre}\in \mathbb{R}^{1\times n}$分别为嵌入（其对偶形式${H_l^{post}}^T$）、投影映射。
关于$DenseNet$的讨论将放在后面的小节。

<blockquote class="remark">
  
**残差连接**的初衷是改变层映射的一阶行为，通过增加**恒等映射**以预防梯度消失问题;形式上在零阶行为里**直接混合**了原始特征，可能会带来**特征稀释**的问题，即原始特征主导了**层输出**，使得**特征变换**难以做出贡献。
**超连接**是残差连接的推广，他将原来的**单一路径**推广到**多路径**，并学习路径两种**混合策略**策略。在**层映射**上同时有多个网络在并行探索，对应地需要学习一组**交换矩阵**（$H^{res}$）实现**横向记忆交换**；在**特征变换**上各子网络记忆被聚合后统一变换，再分发回各网络。

  
</blockquote>

<blockquote class="remark">
  
**同质性意为网络中任意层映射$S_i$的特征变换$F_i$形式上都是相同的函数**

</blockquote>

<blockquote class="discussion">
  
**超连接的形式像是网络传播方向上的状态空间模型(e.g. Mamba)**,其基本抽象形式为$$\begin{align}
    h_{t+1}&={A}{h_t}+Bu_t,\\
    y_t&=Ch_t+Du_t,\\
\end{align}$$
可进一步变换为
$$\begin{align}
    y_{t+1}=C\circ Ah_t+(C\circ B+D)u_t
\end{align}$$
令(6)定义了映射$T：U\rightarrow Y,y_{t+1}=T(u_t)$,
不妨令$U\cong Y \cong X$,
则$T$诱导了一个形式上类似超连接的空间序列$\set {X_i}^L_0$
这里不再展开讨论。

</blockquote>

</blockquote>
<blockquote class="chapter">

# 残差流
**残差流**指深层网络特征增量序列$\set {\Delta x_i}_{1}^{L}$,其中
<blockquote class="definition">
  
  $$\Delta x_i=x_i-x_{i-1}$$

</blockquote>

单纯嵌套网络的残差流分析较为繁琐；当层映射含有残差连接时，其**特征变换输出量序列**即为**残差流**，这使得我们可以把带残差连接的深层神经网络看作一个**离散步长的动力系统**，通过分析特征变换的谱，可以讨论网络的**收敛性**、**稳定性**等性质以设计更易于训练的网络结构。
将式$(2)$错位相减可以得到$$\begin{align}
    x_L&=x_l+\sum_{i=l}^{L-1} F(x_i,W_i),\\
\end{align}$$$S_l$的一阶导函数，也即$X_l,X_{l+1}之间的切映射$
$$s_l:T_{x_l}X_l\rightarrow T_{S_l(x_l)}X_{l+1},$$其形式为$$s_l(x_l)[v]:= \frac{d}{dt}S_l(x_l+tv)\mid _{t=0}=\frac{\partial x_{l+1}}{\partial x_{l}}\circ v,$$$(1)、(2)、(3)$三种形式选定局部坐标$x_l$后分别对应
<blockquote class="definition">



$$\begin{align}
    s_1(x_l)&=\frac{\partial {F}}{\partial{x_{l}}},\\
        s_2(x_l)&=I+\frac{\partial {F}}{\partial{x_{l}}},\\
        s_3(x_l)&=H^{res}_l+{{H_l^{post}}^T}\circ \frac{\partial {F}}{\partial
        (H_l^{pre}\tilde{x_{l}})}\circ H_l^{pre},\\
\end{align}$$
</blockquote>

<blockquote class="remark">
  

在选定局部坐标$x_l$并忽略$H^{pre}_l\circ Expand(x_l)$带来的度量畸变时，$\frac{\partial {F}}{\partial
        (H_l^{pre}\tilde{x_{l}})},\frac{\partial {F}}{\partial{x_{l}}}$具有同源谱结构。


</blockquote>

注意到情形$(8)、(9)$，我们只需要关注特征变换本身的形式、谱特征就能控制好整个网络，而情形$(10)$我们需要额外关注${H}$的性质。
我们忽略损失泛函$\mathcal{L}(x_L,\Phi)$与层映射$S(x_l,W_l)$的具体形式，关注优化过程$$\frac{\partial {\mathcal{L}}}{\partial{W_i}}=\frac{\partial {\mathcal{L}}}{\partial{x_L}}\circ \frac{\partial {x_L}}{\partial x_{i+1}}\circ \frac{\partial {x_{i+1}}}{\partial{W_i}}$$
受到深层网络结构影响的部分：

<blockquote class="objective">
  
$$\frac{\partial {x_L}}{\partial{x_{i+1}}}=\prod ^{L-1}_{j=i+1} \frac{\partial {x_{j+1}}}{\partial{x_{j}}}=\prod ^{L-1}_{j=i+1} s_j ,$$

</blockquote>

代入$(8)、(9)$容易发现

<blockquote class="definition">
  


$$
\begin{align}
    J_1&=\prod_{j=i+1}^{L-1} \frac{\partial {F}}{\partial{x_{j}}} \\
    J_2&=I+\sum _{j=i+1}^{L-1} \frac{\partial {F}}{\partial{x_{j}}}+\sum _{i+1\le j_1\le j_2\le L-1} \frac{\partial {F}}{\partial{x_{j_2}}}\circ \frac{\partial {F}}{\partial{x_{j_1}}}+...+J_1\\
    J_3&=\prod _{j=i+1}^{L-1} H^{res}_j+...
\end{align}
$$

</blockquote>



<blockquote class="proposition">
  
  
$$J_2\approx exp(\sum _{j=i+1}^{L-1} \frac{\partial {F}}{\partial{x_{j}}})$$
待补充，可以从丛论角度解释HC

</blockquote>

</blockquote>

<blockquote class="chapter">


# 路径范畴
在上一章节，我们了解了深度网络训练的障碍，并且介绍了几种已有的解决思路，即残差结构。然而，我们仍未知晓这几种结构的本质动机，以及他们之间的关系。在这一章节，我们将以**特征空间**与**特征映射**为主要对象，探讨在残差结构下网络的特征表达与优化性质究竟发生了什么变化。
<blockquote class="section">

## 单纯嵌套深度网络
对于一个单纯嵌套的$L$层深度网络，其可以看作如下组合映射系统

<blockquote class="definition">
  

$$X_0\xrightarrow{F_0} X_1\xrightarrow{F_1}...\xrightarrow{F_{L-1}} X_L,$$

</blockquote>

其中，$X_i\subset\mathbb{R}^d$,$F_i$为含非线性变换网络作用,不妨假设其为$Transformer$中的$FFN$：

<blockquote class="proposal">
  

$$
X_i\xrightarrow{Emb}Z_0\xrightarrow{\sigma}Z_1\xrightarrow{Prj}X_{i+1},
$$
</blockquote>

其中，$  X_i,X_{i+1}\subset X\subset \mathbb{R}^d,Z_1\subset Z_0\subset \mathbb{R}^D,d<D$,我们暂且假设FFN表达的空间关系是这样的;
$    Emb\in \mathbb{R}^{D\times d},Prj\in \mathbb{R}^{d\times D}
$分别为左嵌入算子和左投影算子，$\sigma$是任意逐点非线性映射如$Sigmoid,Tanh,ReLU,SiLU$等。

<blockquote class="remark">

逐点激活函数导数通常小于1.这是单纯嵌套深度网络梯度消失的根本原因。

</blockquote>



</blockquote>

<blockquote class="section">

## 深度残差网络(ResNet)
既然在深度网络中难以直接学习**特征变换**，ResNet选择了学习**特征残差**,
<blockquote class="proposal">
  
$$
\begin{CD}
X_0 @>T_0>> X_0+\Delta_0 @>T_1>> X_0+\Delta_0+\Delta_1 @>T_2>> \cdots @>T_{L-1}>>
X_0+\sum_{i=0}^{L-1}\Delta_i \\
@VF_0VV     @VF_1VV     @VF_2VV\\
\Delta_0  @. \Delta_1 @.\Delta_2
\end{CD}$$
</blockquote>

不难看出，层映射被一个由**特征映射**诱导的**平移变换**取代了。直觉上来说，同样一个特征变换需要更多次层映射表达，这就是提升训练动力的代价。与**特征稀释**的说法相比，这种视角揭示了残差网络的表达形式，我们无需考虑每一层的混合效应，因为更根本的变化是网络的学习模式发生了变化。
然而，带残差结构的网络往往性能会更好，我们应当想到其中存在一个混淆变量——原网络浅层变换难以优化——而不是认为残差网络具有更好的表达能力。
<blockquote class="remark">
  

深度残差网络ResNet实际上定义了一种新的系统,特征映射退化为局部坐标到平移向量场的映射，整体的层映射退化为一种扩散行为，本质上牺牲了表达能力(需要更多的层来表示旋转、仿射等变换)换取训练动力、稳定性与收敛性。
</blockquote>
</blockquote>

<blockquote class="section">

## 宽度残差深度网络(DenseNet)
同理，我们先观察$DenseNet$的直接展开形态。
<blockquote class="proposal">
  

$$
\begin{align*}
    X_0\xrightarrow{F_0} X_1\xrightarrow{F_1}X_2\xrightarrow{F_2} X_3\xrightarrow{F_3} X_4\xrightarrow{F_4}&...\xrightarrow{F_{L-1}} X_L,\\
    X_0\xrightarrow{F_1} X_0^{1!}\xrightarrow{F_2}X_0^{2!}\xrightarrow{F_3} X_0^{3!}\xrightarrow{F_4}&...\xrightarrow{F_{L-1}} X_0^{\frac{{(L-1)}!}{0!}},\\
    X_0\xrightarrow{F_2}X_0^{\frac{2!}{1!}}\xrightarrow{F_3} X_0^{\frac{3!}{1!}}\xrightarrow{F_4}&...\xrightarrow{F_{L-1}} X_0^{\frac{{(L-1)}!}{1!}},\\
     X_1\xrightarrow{F_2}X_1^{\frac{2!}{1!}}\xrightarrow{F_3} X_1^{\frac{3!}{1!}}\xrightarrow{F_4}&...\xrightarrow{F_{L-1}} X_1^{\frac{{(L-1)}!}{1!}},\\
    X_0\xrightarrow{F_3}X_0^{\frac{3!}{2!}}\xrightarrow{F_4} &...\xrightarrow{F_{L-1}} X_0^{\frac{{(L-1)}!}{2!}},\\
    X_1\xrightarrow{F_3}X_1^{\frac{3!}{2!}}\xrightarrow{F_4} &...\xrightarrow{F_{L-1}} X_1^{\frac{{(L-1)}!}{2!}},\\
    X_2\xrightarrow{F_3}X_1^{\frac{3!}{2!}}\xrightarrow{F_4} &...\xrightarrow{F_{L-1}} X_2^{\frac{{(L-1)}!}{2!}},\\
    X_0^1\xrightarrow{F_3}X_1^{\frac{3!}{2!}1!}\xrightarrow{F_4} &...\xrightarrow{F_{L-1}} X_1^{\frac{{(L-1)}!}{2!}1!},\\
    &...\\
    &X_0\xrightarrow{F_{L-1}} X_0^{L-1},\\
    &X_1\xrightarrow{F_{L-1}} X_1^{L-1},\\
\end{align*}
$$

</blockquote>


为了简洁，用指数表达映射对特征的作用$F$按下指标复合。
$DenseNet$最后的输出是对所有末端元素进行聚合，如池化、MLP方法等。

<blockquote class="remark">
  

来自路径范畴的一个朴素结论是：如果我们规定了每层的特征变换，那么整个网络的复杂行为可以被唯一确定。我们也可以考虑设计一种路径等价类压缩方法来缓解网络过宽的问题。
</blockquote>

仅仅从**特征空间**演化的角度看，$DenseNet$的动机是平凡的，它仅仅是过分地重用浅层的特征，我们难以想象末端输出聚合它们的意义。
然而，我们可以换个角度，从**特征变换**的角度，也即考虑它对训练动力的改造。
给定一个任意层序号$l$我们考察它接收了哪些梯度流，容易想到它们是$\bold{Proposal 3.3}$中起点为$F_i$两端**特征空间**的路径,共有$2^{l+1}$条

<blockquote class="proposal">
  
  $$\begin{CD}
    s
  \end{CD}$$

</blockquote>


</blockquote>

</blockquote>

<blockquote class="chapter">

# 4.向量丛
在第三章，我们简单讨论了深度残差、宽度残差解决深度网络训练动力不足的思路，两者的方案都有些极端，我们不妨考虑一种折中的办法：一种能同时保证训练动力和特征表达的方法。
我们接下来从向量丛的角度考虑HyperConnection架构的底层逻辑。
## 4.1底空间，纤维与截面

</blockquote>
<blockquote class="chapter">

# 超连接残差流的梯度控制问题
不妨假设我们真的需要HC的网络结构，那么其中的$\mathcal{H}^{Res}_l$带来的梯度流稳定性问题显然是值得考察的。
## 5.1 矩阵范数
下面先介绍几种向量$p$-范数定义的矩阵范数。
### 5.1.1 矩阵诱导范数
反映矩阵作为一个线性变换的特征
<blockquote class="definition">

\[
\|A\|_p = \sup_{x \neq 0} \frac{\|Ax\|_p}{\|x\|_p}.
\]
</blockquote>

<blockquote class="example">



- \(p = 2\): 谱范数 (最大的奇异值) \(\|A\|_2 = \sigma_{\max}(A)\)  
- \(p = 1\): 最大列和 \(\|A\|_1 = \max_j \sum_i |A_{ij}|\)  
- \(p = \infty\): 最大行和 \(\|A\|_\infty = \max_i \sum_j |A_{ij}|\)    

</blockquote>

<blockquote class="theorem">
  
 
$$\|A\|_2\le\sqrt{\|A\|_1\|A\|_\infty}$$

</blockquote>

**Theorem 5.1** 为我们提供了一种更简单的梯度控制方案，我们无需进行复杂的奇异值分解和KKT条件，仅需要计算行、列和就能控制线性变换的有界性。然而，我们仍需要进行矩阵乘法得到复合链上每个线性变换的具体形式，这一点可以被双随机矩阵的**闭合性**化简。

### 5.1.2 Schatten 范数
反映矩阵作为空间上的算子的特性，也称**核范数**。
Schatten范数的定义依赖于矩阵的奇异值
<blockquote class="definition">
  

  \[
\|A\|_{\mathcal S_p} = \left( \sum_{i=1}^{r} \sigma_i(A)^p \right)^{1/p}, \quad 1 \le p < \infty,
\]其中，\(\sigma_1 \ge \dots \ge \sigma_r\) 是\(A\)的奇异值向量。  
</blockquote>

<blockquote class="example">
  

- \(p = 2\): 弗罗贝尼乌斯(Frobenius)范数 \(\|A\|_F = \sqrt{\sum_i \sigma_i^2}\)  
- \(p = \infty\): 谱范数 \(\|A\|_{\mathcal S_\infty} = \sigma_{\max}(A) = \|A\|_2\)  
</blockquote>

## 5.2 双随机矩阵半群
<blockquote class="warning">
  
  **注意**:最优传输问题中的双随机矩阵可以是非方阵，为了方便我们讨论$HyperConnection$中的自同态映射$\mathcal{H}^{Res}$,这里考虑一种特殊情况。

</blockquote>
<blockquote class="definition">
  

  $\mathcal{D}_n:=\set {\mathcal{A}\in \mathbb{R}^{n\times n}|\forall i,j\in \mathbb{Z}_n,\boldsymbol{1}^T\mathcal{A_{·j}}=\mathcal{A}_{i·}\boldsymbol{1}=1,\mathcal{A}_{ij}\ge0}$
</blockquote>


<blockquote class="theorem">
  
 Birkhoff-Von Neumann Theorem
  $$\mathcal{D}_n=conv\set{\Pi_n},其中\Pi_n:=\set{P\in \mathcal{D}_n|P_{ij}\in \mathbb{Z}_2}$$$\mathcal{D}_n$是含$n!$个$P_i\in\Pi_n$极点的凸多面体。


</blockquote>

<blockquote class="property">
  

  $$\begin{align*}
    &dim(\mathcal{D}_n)=n^2-(2n-1)=(n-1)^2,n>2时\mathcal{D}_n不是单纯形,\\\
    &(\mathcal{D}_n,·)是一个关于矩阵乘法的\bold{幺半群},单位元是I_n,\\
    &置换矩阵群(\Pi_n,·)\cong(S_n,\circ)是(\mathcal{D}_n,·)的\bold{单位群},\\
    &根据\bold{Theorem 5.1},谱范数\|\mathcal{A}\|_2\le1,\\
    &一定存在大小为1的奇异值，特征向量为\boldsymbol{1}_n\\
    &内点\mathcal{D}_n^{\circ}=\set{\mathcal{A}\in \mathcal{D}_n|\mathcal{A}_{ij}>0}=\mathcal{D}_n\cap \mathbb{R}_+^{n\times n}\\
    &唯一不动点、最大内切球中心\mathcal{A}_{Uni}=\frac{1}{n}\boldsymbol{1}_n\boldsymbol{1}_n^T\\
  \end{align*}$$

</blockquote>
现在，我们找到了谱范数有界，且关于矩阵乘法闭合的一族矩阵，我们只需要保证每层线性变换都属于这族矩阵，而无需显示计算复合映射链上每个矩阵的具体形式，更不用进行大规模SVD。

<blockquote class="tip">

我们不妨再考虑$\Pi_n$上关于的凸组合标量作用的空间，可以从复链形的角度，并结合HC中流的意义，考虑如何选择合适的规模参数$n$。
</blockquote>

<blockquote class="discussion">


不难看出，双随机矩阵的矩阵诱导-1范数和矩阵诱导-无穷范数均为1，其谱范数上确界为1；其次，双随机矩阵流形是闭合的，其上任意元素关于矩阵乘法复合仍为双随机矩阵，从而保证了式$(13)$中第一项复合算子作用是不会发散的。我们还可以再详细讨论双随机矩阵的代数、闭合性、有界性、紧致性，并探索$End(n)$中是否存在其他乘法子群也具有这样的谱特征。
注意到$\mathcal{D}_n$的最大奇异值必为1，因此讨论其奇异值下界并考虑施加约束会更有意义。$\mathcal{D}_n$是凸集，单纯同调只能用于证明优化是无拓扑障碍的，这显然能直接从凸性质看出它的各阶连通性；因此可以考虑奇异同调的角度，构造对$\mathcal{D}_n$三角剖分得到单纯形并构造单形间映射与元素到谱元素的映射，得到链复形。
 

</blockquote>



## 5.2 双随机矩阵的生成方法
当我们考虑获取一个双随机矩阵$\mathcal{A}\in\mathcal{D}_n$时，最直接的办法是用$\mathcal{D}_n$的顶点表示
<blockquote class="proposal">

$$\begin{align*}
  &\mathcal{A}=\sum_{i=1}^{n!}\lambda_i·P_i,\\
  其中P_i\in \Pi_n,&\lambda_i\in\mathbb{R}_{\ge0},\sum\lambda_i=1,\forall i,j,P_i\ne P_j,
\end{align*}$$

</blockquote>

这种方法的主要障碍是需要存储$n!$个$\mathbb{R}^{n\times n}$上的元素，尽管置换矩阵是稀疏的，但数量仍造成了困难。
<blockquote class="remark">

  此外，这种表示仅仅是存在性凸分解，同一个$\mathcal{A}$在$span(\Pi_n)$下的表示是不唯一的，因为$dim(\mathcal{D}_n)=(n-1)^2$;而寻找$(n-1)^2$线性无关的$P$似乎仍无法解决问题，这里不展开讨论
</blockquote>

我们需要一种更简单的生成方法，并且希望得到的$\mathcal{A}\in\mathcal{D}_n^{\circ}$以尽量保证**任务损失梯度**不会轻易使得$\mathcal{A}$脱离$\mathcal{D}_n$。


### 5.2.1 Sinkhorn算法
我们首先考虑在双边边际均为1的最优传输问题中，Sinkhorn构造$\mathcal{A}\in \mathcal{D}_n^{\circ}$的方案。
<blockquote class="algorithm">

**Entropy-Regularized Sinkhorn**

| Step | Description |
|---|---|
| Input | Cost matrix \(C \in \mathbb{R}^{n\times n}_+\), regularization \(\varepsilon>0\), iterations \(T\) |
| Init | \(u \leftarrow \boldsymbol{1},\; v \leftarrow \boldsymbol{1}\) |
| Precompute | \(K \leftarrow \exp(-C/\varepsilon)\) |
| Loop | for \(k = 1,\dots,T\) |
|  | \(u \leftarrow \boldsymbol{1} \oslash (K v)\) |
|  | \(v \leftarrow \boldsymbol{1} \oslash (K^\top u)\) |
| Output | \(\mathcal{A} = \mathrm{diag}(u) K \mathrm{diag}(v)\) |
</blockquote>

其中，$\oslash$是逐元素除法。

<blockquote class="objective">

 \(\mathcal{A}\) is the solution of the entropy-regularized optimal transport problem:
\[
\min_{\mathcal{A} \in \mathcal{D}_n} \langle \mathcal{A}, C\rangle_{\mathcal{F}} - \varepsilon H(\mathcal{A}), \quad H(\mathcal{A}) = -\sum_{i,j} \mathcal{\mathcal{A}}_{ij} \ln \mathcal{A}_{ij}.
\]

</blockquote>
<blockquote class="remark">
  
在HC梯度控制问题中，我们只是想利用双随机矩阵的复合稳定性，似乎并不存在一个明确的代价矩阵。

</blockquote>

任意给定一个$M\in \mathbb{R}^{n\times n}$,我们可以通过逐元素激活函数得到正锥元素$K$,不失一般性的地
<blockquote class="proposal">
  
  $$\begin{align}
    Exponential:K_{ij}&=\alpha^{\tau M_{ij}}\\
    SoftPlus:K_{ij}&=\frac{1}\tau log_{\alpha}(1+\alpha^{\tau M_{ij}})
  \end{align}
  $$
</blockquote>

通常$\alpha$取自然底数$e$,$\tau$是温度系数,注意到$(15)$的导数是有界的。
我们考虑Sinkhorn对应的优化目标，根据最大熵原理，其中元素级熵正则的作用是使得$\mathcal{A}$逼近一个均匀双随机矩阵$\mathcal{A}_{Uni}=\frac{1}{n}\boldsymbol{1}_n\boldsymbol{1}_n^T$

</blockquote>